{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269212fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV \n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier \n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import ParameterGrid, StratifiedKFold, RepeatedStratifiedKFold, train_test_split \n",
    "import matplotlib.pyplot as plt import seaborn as sns\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn import preprocessing\n",
    "from mrmr import mrmr_classif\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import LinearSVC as SVM\n",
    "from sklearn.svm import SVC as SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(rc={\"figure.figsize\": (10, 8), \"font.size\": 10}) sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"talk\", font_scale=1.0, rc={\"lines.linewidth\": 2.5})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804ba798",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2691fb28",
   "metadata": {},
   "source": [
    "Load feature dataframes of three observers and select robust features under different observer annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5720949",
   "metadata": {},
   "outputs": [],
   "source": [
    "wilms_features = pd.read_csv ('/Users/koska/Downloads/wilms_features.csv') \n",
    "nb_features = pd.read_csv('/Users/koska/Downloads/nb_features.csv') \n",
    "wilms_features_obs2 = pd.read_csv('/Users/koska/Downloads/wilms_features_obs2.csv') \n",
    "nb_features_obs2 = pd.read_csv('/Users/koska/Downloads/nb_features_obs2.csv') \n",
    "wilms_featurs_obs3 = pd.read_csv('/Users/koska/Downloads/wilms_features_obs3.csv') \n",
    "nb_features_obs3 = pd.read_csv('/Users/koska/Downloads/nb_features_obs3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b899d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_obs_nb_obs1 = nb_features [:18] \n",
    "inter_obs_nb_obs2 = nb_features_obs2 [:18] \n",
    "inter_obs_nb_obs3 = nb_features_obs3 [:18] \n",
    "nb_label = np.zeros ((18))\n",
    "\n",
    "inter_obs_wilms_obs1 = wilms_features [:22] \n",
    "inter_obs_wilms_obs2 =wilms_features_obs2[:22] \n",
    "inter_obs_wilms_obs3 = wilms_featurs_obs3[:22] \n",
    "wilms_label = np.ones ((22))\n",
    "\n",
    "obs1_df = pd.concat ((inter_obs_nb_obs1,inter_obs_wilms_obs1),axis = 0) \n",
    "obs2_df = pd.concat ((inter_obs_nb_obs2,inter_obs_wilms_obs2),axis = 0) \n",
    "obs3_df = pd.concat ((inter_obs_nb_obs3,inter_obs_wilms_obs3),axis = 0)\n",
    "\n",
    "\n",
    "icc_obs1_obs2 = obs1_df.corrwith (other=obs2_df) \n",
    "robust_features_12 =icc_obs1_obs2 [icc_obs1_obs2 >=0.9]\n",
    "icc_obs1_obs3 = obs1_df.corrwith (other=obs3_df) \n",
    "robust_features_13 =icc_obs1_obs3 [icc_obs1_obs3 >=0.9]\n",
    "icc_obs2_obs3 = obs2_df.corrwith (other= obs3_df) \n",
    "robust_features_23 =icc_obs2_obs3 [icc_obs2_obs3 >=0.9]\n",
    "robust_features_obs1_2= main_feature_df.loc[:,robust_features_12.index] \n",
    "robust_features_obs1_3 =main_feature_df.loc[:,robust_features_13.index] \n",
    "robust_features_obs2_3 =main_feature_df.loc[:,robust_features_23.index]\n",
    "\n",
    "s1 = set(robust_features_obs1_2)\n",
    "s2 = set(robust_features_obs1_3)\n",
    "s3 = set(robust_features_obs2_3)\n",
    "set12_13 = s1.intersection(s2)\n",
    "set13_23 = s2.intersection(s3) \n",
    "result_set = set12_13 .intersection(s3) \n",
    "common12_13 = list (set12_13) \n",
    "common12_23 = list (set12_23) \n",
    "common13_23 = list (set13_23) \n",
    "final_list = list(result_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0eba0f",
   "metadata": {},
   "source": [
    "Then select non redundant and high variance features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be25f1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_non_redundan (corr_columns, robust_data,ext_data ):\n",
    "    columns = np.full((corr_columns.shape[0],), True, dtype=bool) \n",
    "    for i in range(corr_columns.shape[0]):\n",
    "        for j in range(i+1, corr_columns.shape[0]): \n",
    "            if abs (corr_columns.iloc[i,j]) >= 0.9:\n",
    "                if columns[j]: \n",
    "                    columns[j] = False\n",
    "    selected_columns_ = robust_data.columns[columns] \n",
    "    data_ =robust_data[selected_columns_] \n",
    "    ext_data_df = ext_data [selected_columns_] \n",
    "    return data_, ext_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f752fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "hutf_complete_df = pd.read_csv ('/Users/koska/Documents/hutf_complete_df.csv') \n",
    "ext_complete_df = pd.read_csv ('/Users/koska/Documents/external_complete_df.csv')\n",
    "\n",
    "hacettepe_klinik_last_analysis = pd.read_csv ('/Users/koska/Documents/hacettepe_klinik_last_analysis.csv') \n",
    "dm_klinik_last_analysis = pd.read_csv ('/Users/koska/Documents/dm_klinik_last_analysis.csv')\n",
    "\n",
    "hutf_labels = hutf_complete_df ['label']\n",
    "ext_labels = ext_complete_df ['label']\n",
    "\n",
    "hütf_just_features = hutf_complete_df.drop(['label','Patient code'],axis = 1) \n",
    "ext_just_features = ext_complete_df.drop(['label','Patient code'],axis = 1)\n",
    "\n",
    "hutf_robust_features = hütf_just_features [final_list]\n",
    "ext_robust_features = ext_just_features [final_list]\n",
    "\n",
    "hutf_nonredundan_features = hutf_robust_features.corr()\n",
    "non_redundan_hütf_df, non_redundan_ext_df = select_non_redundan (hutf_nonredundan_features ,hutf_robust_features,\n",
    "                                                                 ext_robust_features)\n",
    "                                                                 \n",
    "\n",
    "sel_hutf = VarianceThreshold(threshold=0.05)\n",
    "selection_hutf = sel_hutf.fit_transform(non_redundan_hütf_df)\n",
    "concol_hutf = [column for column in non_redundan_hütf_df.columns if column not in non_redundan_hütf_df.columns[sel_hutf.get_support()]] \n",
    "hutf_var_dropped = non_redundan_hütf_df.drop(concol_hutf,axis=1)\n",
    "ext_var_droppedd = non_redundan_ext_df.drop (concol_hutf,axis =1)                                                                 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0f9fc5",
   "metadata": {},
   "source": [
    "Now since we have robust, nonredundant and high variance features, we can proceed with supervised feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007a72fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_express(X_p, X_n):\n",
    "    n_p = len(X_p) \n",
    "    n_n = len(X_n)\n",
    "    t, p = stats.ttest_ind(X_p, X_n, axis=0, equal_var=False)\n",
    "    mu_p = X_p.mean(axis=0) \n",
    "    mu_n = X_n.mean(axis=0) \n",
    "    mean_diff = mu_p - mu_n \n",
    "    var_p = X_p.var(axis=0) \n",
    "    var_n = X_n.var(axis=0)\n",
    "    S_pooled = np.sqrt( ((n_p-1)*var_p + (n_n-1)*var_n)/(n_p + n_n-2) ) \n",
    "    Cd = np.divide(mean_diff,S_pooled)\n",
    "    thr = (mu_p * np.sqrt(var_n) + mu_n * np.sqrt(var_p)) / (np.sqrt(var_n) + np.sqrt(var_p)) \n",
    "    n_above = np.sum(X_n > thr, axis=0)\n",
    "    p_below = np.sum(X_p < thr, axis=0)\n",
    "    mc_rate = 0.5* ( n_above / n_n + p_below / n_p)\n",
    "    return p, Cd, mc_rate\n",
    "\n",
    "\n",
    "def volcano(dat, col_label='label'): \n",
    "    positive = dat[dat[col_label]==1].drop(col_label,axis=1).values \n",
    "    n_p = len(positive)\n",
    "    negative = dat[dat[col_label]==0].drop(col_label, axis=1).values \n",
    "    n_n = len(negative)\n",
    "    pval, cd, mcr = diff_express(positive, negative)\n",
    "    arr = np.hstack((pval.reshape(-1,1),mcr.reshape(-1,1)))\n",
    "    df_ = pd.DataFrame(arr, index=list(dat.columns)[:-1], columns=['pval','mc_rate'])\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed68915c",
   "metadata": {},
   "source": [
    "First scale the feature set with standart scaler. Then apply volcano, mrmr, rfe and filter based feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad94618",
   "metadata": {},
   "outputs": [],
   "source": [
    "hutf_scaler = preprocessing.StandardScaler().fit(hutf_var_dropped) \n",
    "hutf_scaled = hutf_scaler.transform (hutf_var_dropped)\n",
    "ext_scaled = hutf_scaler.transform (ext_var_droppedd)\n",
    "hutf_scaled_df = pd.DataFrame (hutf_scaled, columns = hutf_var_dropped.columns ) \n",
    "hutf_scaled_df_with_labels = hutf_scaled_df.copy ()\n",
    "hutf_scaled_df_with_labels ['label'] = hutf_labels\n",
    "ext_scaled_df = pd.DataFrame (ext_scaled, columns = ext_var_droppedd.columns ) \n",
    "ext_scaled_df_with_labels = ext_scaled_df.copy ()\n",
    "ext_scaled_df_with_labels ['label'] = ext_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96b3c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_v_hutf = volcano(hutf_scaled_df_with_labels)\n",
    "\n",
    "df_v_hutf['log10p'] = df_v_hutf ['pval'].apply(lambda x: -np.log10(x))\n",
    "volcano_selected_hutf = df_v_hutf[(df_v_hutf['log10p'] > 4.) & (df_v_hutf['mc_rate'] < 0.3)]\n",
    "volcano_selected_columns_hutf = list (volcano_selected_hutf.index)\n",
    "X_volcano_features_hutf = hutf_scaled_df [volcano_selected_columns_hutf] \n",
    "X_volcano_features_hutf ['label'] = np.array (hutf_labels) \n",
    "X_volcano_features_hutf.to_csv ('X_volcano_features_hutf.csv')\n",
    "\n",
    "X_volcano_features_ext = ext_scaled_df [volcano_selected_columns_hutf] \n",
    "X_volcano_features_ext ['label'] = np.array (ext_labels) \n",
    "X_volcano_features_ext.to_csv ('X_volcano_features_ext.csv')\n",
    "\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "sns.scatterplot(data=df_v_hutf, x='mc_rate', y='log10p', alpha=0.8) \n",
    "plt.xlabel('Misclassification Rate')\n",
    "plt.ylabel('log10(pval)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab57b625",
   "metadata": {},
   "outputs": [],
   "source": [
    "mrmr_selected_features_hutf = mrmr_classif(hutf_scaled_df, hutf_labels, K = 6) \n",
    "X_mrmr_features_hutf = hutf_scaled_df[mrmr_selected_features_hutf] \n",
    "X_mrmr_features_ext = ext_scaled_df[mrmr_selected_features_hutf] \n",
    "X_mrmr_features_hutf ['label'] = np.array (hutf_labels) \n",
    "X_mrmr_features_ext ['label'] = np.array (ext_labels) \n",
    "X_mrmr_features_hutf.to_csv ('X_mrmr_features_hutf.csv')\n",
    "X_mrmr_features_ext.to_csv ('X_mrmr_features_ext.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da89c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "rfe_hutf = RFE(model,n_features_to_select=6)\n",
    "fit_hutf = rfe_hutf.fit(hutf_scaled_df, hutf_labels) \n",
    "rfe_selected_hutf = hutf_scaled_df.columns[fit_hutf.support_] \n",
    "X_rfe_features_hutf = hutf_scaled_df[rfe_selected_hutf] \n",
    "X_rfe_features_ext = ext_scaled_df[rfe_selected_hutf] \n",
    "X_rfe_features_hutf ['label'] = np.array (hutf_labels) \n",
    "X_rfe_features_ext ['label'] = np.array (ext_labels) \n",
    "X_rfe_features_hutf.to_csv ('X_rfe_features_hutf.csv') \n",
    "X_rfe_features_ext.to_csv ('X_rfe_features_ext.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42913b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filter_best_features (all_features, labels):\n",
    "    wrap_feat_select = ExtraTreesClassifier ()\n",
    "    wrap_feat_select.fit(all_features, labels)\n",
    "    train_pd = pd.DataFrame (all_features)\n",
    "    dfcolumns = pd.DataFrame (train_pd.columns)\n",
    "    feat_importances = pd.Series (wrap_feat_select.feature_importances_, index = dfcolumns) feat_importances = feat_importances.nlargest (6)\n",
    "    return feat_importances\n",
    "def get_filter_reduced (feat_importances,all_features): \n",
    "    wrapper_index= feat_importances.index \n",
    "    reduced_wrapper= all_features [:,wrapper_index] \n",
    "    return reduced_wrapper\n",
    "def plot_feat_importances (feat_importances): \n",
    "    feat_importances.plot (kind = 'barh') \n",
    "    plt.show ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47054c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_features_hutf = get_filter_best_features (hutf_scaled_df, hutf_labels) \n",
    "filter_features_hutf_ = list (filter_features_hutf.index)\n",
    "filter_features_= [ filter_features_hutf_[i][0] for i in range(len (filter_features_hutf_)) ] \n",
    "X_filter_features_hutf = hutf_scaled_df [filter_features_]\n",
    "X_filter_features_ext = ext_scaled_df [filter_features_] \n",
    "X_filter_features_hutf ['label'] = np.array (hutf_labels) \n",
    "X_filter_features_ext ['label'] = np.array (ext_labels) \n",
    "X_filter_features_hutf.to_csv ('X_filter_features_hutf.csv') \n",
    "X_filter_features_ext.to_csv ('X_filter_features_ext.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d6dded",
   "metadata": {},
   "source": [
    "Since we completed feature selection, we can train our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf85aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_four (dict1,dict2,dict3,dict4):\n",
    "    res = {**dict1, **dict2, **dict3, **dict4} \n",
    "    return res\n",
    "\n",
    "def get_svm_scores_cv (df):\n",
    "    labels,features = set_features_labels (df)\n",
    "    scoring = ('f1', 'recall', 'precision', 'accuracy', 'roc_auc')\n",
    "    cv = RepeatedStratifiedKFold(n_splits=4, n_repeats=10, random_state=1)\n",
    "    svm = SVM(max_iter=10000)\n",
    "    svm_over = SVM(max_iter=10000)\n",
    "    clf_rbf = SVC(kernel=\"rbf\",C=5,max_iter=10000)\n",
    "    clf_weight = SVC(kernel=\"linear\", max_iter=10000,class_weight='balanced')\n",
    "    oversample = SMOTE()\n",
    "    over_X, over_y = oversample.fit_resample(features,labels)\n",
    "    scores_svm = cross_validate(svm, features, labels, scoring=scoring, cv=cv) scores_svm_over = cross_validate(svm_over, over_X, over_y, scoring=scoring, cv=cv) scores_svm_rbf = cross_validate(clf_rbf, features, labels, scoring=scoring, cv=cv) scores_svm_weight = cross_validate(clf_weight, features, labels, scoring=scoring, cv=cv)\n",
    "    svm_dict = build_scores_dict (scores_svm, 'svm')\n",
    "    svm_over_dict = build_scores_dict (scores_svm_over, 'over')\n",
    "    svm_rbf_dict = build_scores_dict (scores_svm_rbf, 'rbf')\n",
    "    svm_weight_dict = build_scores_dict (scores_svm_weight, 'weighted') result_dict = merge_four (svm_dict,svm_over_dict,svm_rbf_dict,svm_weight_dict)\n",
    "    #print_cv_scores (scores_svm,'svm')\n",
    "    #print_cv_scores (scores_svm_over, 'over')\n",
    "    #print_cv_scores (scores_svm_rbf, 'rbf')\n",
    "    #print_cv_scores (scores_svm_weight, 'weighted')\n",
    "    return result_dict\n",
    "\n",
    "def get_rf_scores_cv (df):\n",
    "    labels,features = set_features_labels (df)\n",
    "    scoring = ('f1', 'recall', 'precision', 'accuracy', 'roc_auc')\n",
    "    cv = RepeatedStratifiedKFold(n_splits=4, n_repeats=10, random_state=1)\n",
    "    rf_model = RandomForestClassifier(n_estimators=150, random_state= 10)\n",
    "    rf_over = RandomForestClassifier(n_estimators=150, random_state= 10) balanced_rf_model = BalancedRandomForestClassifier(n_estimators=150, random_state=2) dec_tree_model = DecisionTreeClassifier()\n",
    "    oversample = SMOTE()\n",
    "    over_X, over_y = oversample.fit_resample(features,labels)\n",
    "    scores_rf = cross_validate(rf_model, features, labels, scoring=scoring, cv=cv)\n",
    "    scores_rf_over = cross_validate( rf_over, over_X, over_y, scoring=scoring, cv=cv) scores_rf_balanced = cross_validate( balanced_rf_model, features, labels, scoring=scoring, cv=cv) scores_dec_tree = cross_validate(dec_tree_model, features, labels, scoring=scoring, cv=cv)\n",
    "    rf_dict = build_scores_dict (scores_rf, 'RF')\n",
    "    rf_over_dict = build_scores_dict (scores_rf_over, 'over')\n",
    "    rf_balanced_dict = build_scores_dict (scores_rf_balanced, 'balanced') dec_tree_dict = build_scores_dict (scores_dec_tree, 'dec_tree')\n",
    "    result_dict = merge_four (rf_dict,rf_over_dict,rf_balanced_dict,dec_tree_dict)\n",
    "    #print_cv_scores (scores_rf,'RF') #print_cv_scores (scores_rf_over, 'over') #print_cv_scores (scores_rf_balanced, 'balanced') #print_cv_scores (scores_dec_tree, 'DT')\n",
    "    return result_dict\n",
    "\n",
    "def set_features_labels (df):\n",
    "    labels = df ['label']\n",
    "    features = df.drop ('label', axis =1) \n",
    "    if 'Unnamed: 0' in features.keys():\n",
    "        features = features.drop ('Unnamed: 0', axis=1) \n",
    "    else:\n",
    "        features = features \n",
    "    return labels, features\n",
    "\n",
    "def print_scores (labels, preds,pref):\n",
    "    print(str(pref) + 'Recall: %.3f' % recall_score(labels, preds)) \n",
    "    print(str(pref) +'Accuracy: %.3f' % accuracy_score(labels, preds)) \n",
    "    print(str(pref) +'F1 Score: %.3f' % f1_score(labels, preds)) \n",
    "    print(str(pref) +'Precision: %.3f' % precision_score(labels, preds)) \n",
    "    print(str(pref) +'ROC_AUC: %.3f' % roc_auc_score(labels, preds))\n",
    "\n",
    "def print_cv_scores (scores,pref):\n",
    "    print(str(pref) + 'Mean f1: %.3f' % mean(scores['test_f1'])) \n",
    "    print(str(pref) + 'Std f1: %.3f' % stdev(scores['test_f1'])) \n",
    "    print(str(pref) + 'Mean recall: %.3f' % mean(scores['test_recall'])) \n",
    "    print(str(pref) + 'Std recall: %.3f' % stdev(scores['test_recall'])) \n",
    "    print(str(pref) + 'Mean precision: %.3f' % mean(scores['test_precision'])) \n",
    "    print(str(pref) + 'Std precision: %.3f' % stdev(scores['test_precision'])) \n",
    "    print(str(pref) + 'Mean accuracy: %.3f' % mean(scores['test_accuracy'])) \n",
    "    print(str(pref) + 'Std accuracy: %.3f' % stdev(scores['test_accuracy'])) \n",
    "    print(str(pref) + 'Mean roc_auc: %.3f' % mean(scores['test_roc_auc'])) \n",
    "    print(str(pref) + 'Std roc_auc: %.3f' % stdev(scores['test_roc_auc']))\n",
    "\n",
    "def build_scores_dict (scores,pref):\n",
    "    scores_dict= {str(pref) + '_Mean f1:':mean(scores['test_f1']),\n",
    "    str(pref) + '_Std f1:': stdev(scores['test_f1']),\n",
    "    str(pref) + '_Mean recall:': mean(scores['test_recall']), \n",
    "    str(pref) + '_Std recall:': stdev(scores['test_recall']), \n",
    "    str(pref) + '_Mean precision:': mean(scores['test_precision']), \n",
    "    str(pref) + '_Std precision:': stdev(scores['test_precision']), \n",
    "    str(pref) + '_Mean accuracy:': mean(scores['test_accuracy']), \n",
    "    str(pref) + '_Std accuracy:': stdev(scores['test_accuracy']), \n",
    "    str(pref) + '_Mean roc_auc:': mean(scores['test_roc_auc']), \n",
    "    str(pref) + '_Std roc_auc:': mean(scores['test_roc_auc'])}\n",
    "    return scores_dict\n",
    "\n",
    "def get_svm_scores (df):\n",
    "    labels,features = set_features_labels (df)\n",
    "    svm = SVM(max_iter=10000)\n",
    "    svm_over = SVM(max_iter=10000)\n",
    "    #clf = svm.SVC(kernel=\"linear\", C=1.0)\n",
    "    clf_rbf = SVC(kernel=\"rbf\",C=5,max_iter=10000)\n",
    "    clf_weight = SVC(kernel=\"linear\", max_iter=10000,class_weight='balanced')\n",
    "    oversample = SMOTE()\n",
    "    over_X, over_y = oversample.fit_resample(features,labels)\n",
    "    otr_features, ote_features, otr_labels, ote_labels = train_test_split(over_X, over_y, test_size=0.1, stratify=over_y)\n",
    "    tr_features, te_features, tr_labels, te_labels = train_test_split(features, labels, test_size=0.1, stratify=labels)\n",
    "    svm.fit(tr_features, tr_labels) svm_over.fit (otr_features, otr_labels) \n",
    "    clf_rbf.fit (tr_features, tr_labels) \n",
    "    #clf.fit(X, y) clf_weight.fit(tr_features, tr_labels)\n",
    "    y_pred_svm = svm.predict(te_features) \n",
    "    y_pred_clf_rbf = clf_rbf.predict(te_features) \n",
    "    y_pred_weight = clf_weight.predict(te_features) \n",
    "    y_pred_over = svm_over.predict (ote_features)\n",
    "    #print_scores (te_labels,y_pred_svm,'svm')\n",
    "    #print_scores (te_labels,y_pred_clf_rbf, 'rbf')\n",
    "    #print_scores (te_labels,y_pred_weight, 'weighted')\n",
    "    #print_scores (ote_labels, y_pred_over, 'over')\n",
    "    return y_pred_svm,y_pred_clf_rbf,y_pred_weight,y_pred_over, svm,clf_rbf, clf_weight,svm_over\n",
    "\n",
    "def get_rf_scores (df):\n",
    "    labels,features = set_features_labels (df)\n",
    "    rf_model = RandomForestClassifier(n_estimators=150, random_state= 10)\n",
    "    rf_over = RandomForestClassifier(n_estimators=150, random_state= 10) balanced_rf_model = BalancedRandomForestClassifier(n_estimators=150, random_state=2) dec_tree_model = DecisionTreeClassifier()\n",
    "    oversample = SMOTE()\n",
    "    over_X, over_y = oversample.fit_resample(features,labels)\n",
    "    rf_model.fit(features, labels)\n",
    "    balanced_rf_model.fit (features, labels)\n",
    "    rf_over.fit (over_X, over_y)\n",
    "    dec_tree_model.fit(features, labels)\n",
    "    y_pred_rf = rf_model.predict(features) \n",
    "    y_pred_balanced_rf = balanced_rf_model.predict(features) \n",
    "    y_pred_dec_tree = dec_tree_model.predict(features) \n",
    "    y_pred_over = rf_over.predict (over_X)\n",
    "    #print_scores (labels,y_pred_rf,'RandomForest') \n",
    "    #print_scores (labels,y_pred_balanced_rf, 'BRF') \n",
    "    #print_scores (labels,y_pred_dec_tree, 'DecTree') \n",
    "    #print_scores (over_y, y_pred_over, 'Over')\n",
    "    return y_pred_rf,y_pred_balanced_rf,y_pred_dec_tree,y_pred_over,rf_model,balanced_rf_model, dec_tree_model, rf_over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0a0204",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_svm_hutf_filter =get_svm_scores_cv (X_filter_features_hutf) \n",
    "cv_svm_hutf_mrmr =get_svm_scores_cv (X_mrmr_features_hutf) \n",
    "cv_svm__hutf_rfe =get_svm_scores_cv (X_rfe_features_hutf) \n",
    "cv_svm_hutf_volcano =get_svm_scores_cv (X_volcano_features_hutf)\n",
    "\n",
    "cv_rf_hutf_filter =get_rf_scores_cv (X_filter_features_hutf) \n",
    "cv_rf_hutf_mrmr =get_rf_scores_cv (X_mrmr_features_hutf) \n",
    "cv_rf_hutf_rfe =get_rf_scores_cv (X_rfe_features_hutf) \n",
    "cv_rf_hutf_volcano =get_rf_scores_cv (X_volcano_features_hutf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5333d4",
   "metadata": {},
   "source": [
    "Use best model and best feature selection strategy to test with external validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1885bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "external_valid_scores = get_svm_scores (X_rfe_features_ext)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dade92af",
   "metadata": {},
   "source": [
    "Now let's combine all the best radiomics features selected by 4 different supervised feature selection methods and clinical features. Then reapply secondary feature selection to find best of best subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593a9a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "union_list = [volcano_selected_columns_hutf,mrmr_selected_features_hutf, X_rfe_features_hutf, X_filter_features_hutf] \n",
    "feature_union = set ().union (*union_list)\n",
    "\n",
    "hutf_temp = hutf_scaled_df.copy ()\n",
    "hutf_temp ['label'] = np.array (hutf_labels) \n",
    "union_df = hutf_temp[feature_union]\n",
    "\n",
    "hutf_clinic_temp = hacettepe_klinik_last_analysis.copy () \n",
    "hutf_clinic_temp = hutf_clinic_temp.drop (['label'],axis = 1) \n",
    "hutf_clinic_temp = hutf_clinic_temp.reset_index(drop=True)\n",
    "clinico_rad_hutf_raw = pd.concat ((hutf_clinic_temp,union_df),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290d4501",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "rfe_clinico_rad = RFE(model,n_features_to_select=6)\n",
    "fit_clinico_rad = rfe_clinico_rad.fit(clinico_rad_hutf_raw, np.array (hutf_labels)) \n",
    "rfe_selected_clinico_rad = clinico_rad_hutf_raw.columns[fit_clinico_rad.support_] \n",
    "X_rfe_features_clinico_rad = clinico_rad_hutf_raw[rfe_selected_clinico_rad] \n",
    "X_rfe_features_clinico_rad ['label'] = np.array (hutf_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eceb18dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_temp = ext_scaled_df.copy ()\n",
    "ext_temp ['label'] = np.array (ext_labels) \n",
    "union_ext_df = ext_temp[feature_union]\n",
    "ext_clinic_temp = dm_klinik_last_analysis.copy ()\n",
    "dm_clinic_temp = ext_clinic_temp.drop (['label'],axis = 1) \n",
    "dm_clinic_temp = dm_clinic_temp.reset_index(drop=True)\n",
    "clinico_rad_ext_raw = pd.concat ((dm_clinic_temp,union_ext_df),axis = 1) \n",
    "X_rfe_features_clinico_rad_ext = clinico_rad_ext_raw[rfe_selected_clinico_rad] \n",
    "X_rfe_features_clinico_rad_ext['label'] = np.array (ext_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea6f3ab",
   "metadata": {},
   "source": [
    "First use cross validation to explore performance estimates with the train set, then apply best performing model and features on the external test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077be0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_svm_clin_rad =get_svm_scores_cv (X_rfe_features_clinico_rad) \n",
    "cv_rf_clin_rad =get_rf_scores_cv (X_rfe_features_clinico_rad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cfd6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "external_valid_scores_clinico_radiomic = get_svm_scores (X_rfe_features_clinico_rad_ext)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
